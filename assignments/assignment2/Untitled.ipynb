{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Guide\n",
    "\n",
    "Run the notebook by pressing the Run button in the top UI. This notebook uses Python 3, only with stock imports.\n",
    "\n",
    "## 1.1 Sentiment Analysis (1.5 points)\n",
    "\n",
    "• Given the following short movie reviews and a document D = fast, couple, shoot,\n",
    "fly. Write a python implementation (from scratch!) that compute the most likely\n",
    "class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods. Each review is labeled with a genre, either comedy or action:\n",
    "1. fun, couple, love, love <b>comedy</b>\n",
    "2. fast, furious, shoot <b>action</b>\n",
    "3. couple, fly, fast, fun, fun <b>comedy</b>\n",
    "4. furious, shoot, shoot, fun <b>action</b>\n",
    "5. fly, fast, shoot, love <b>action</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our training/test data\n",
    "training_set = [\n",
    "    (['fun', 'couple', 'love', 'love'], 'comedy'),\n",
    "    (['fast', 'furious', 'shoot'], 'action'),\n",
    "    (['couple', 'fly', 'fast', 'fun'], 'comedy'),\n",
    "    (['furious', 'shoot', 'shoot', 'fun'], 'action'),\n",
    "    (['fly', 'fast', 'shoot', 'love'], 'action')\n",
    "]\n",
    "test_statement = ['fast', 'couple', 'shoot', 'fly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('couple', 'comedy'): 2, ('fun', 'comedy'): 2, ('love', 'comedy'): 1, ('fast', 'action'): 2, ('furious', 'action'): 2, ('shoot', 'action'): 3, ('fast', 'comedy'): 1, ('fly', 'comedy'): 1, ('fun', 'action'): 1, ('fly', 'action'): 1, ('love', 'action'): 1}\n",
      "{'comedy': 2, 'action': 3}\n"
     ]
    }
   ],
   "source": [
    "#Initialise some helper variables\n",
    "dataset = set()\n",
    "likelihood_map_data = {}\n",
    "likelihood_map_labels = {}\n",
    "\n",
    "for x0 in [x for x, y in training_set]:\n",
    "    dataset.update(x0)\n",
    "\n",
    "for x, y in training_set:\n",
    "    if y in likelihood_map_labels:\n",
    "        likelihood_map_labels[y] += 1\n",
    "    else:\n",
    "        likelihood_map_labels[y] = 1\n",
    "    for word in set(x):\n",
    "        if(word, y) in likelihood_map_data:\n",
    "            likelihood_map_data[(word, y)] += 1\n",
    "        else:\n",
    "            likelihood_map_data[(word, y)] = 1\n",
    "\n",
    "print(likelihood_map_data)\n",
    "print(likelihood_map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for label in likelihood_map_labels:\n",
    "    quantity = likelihood_map_labels[label]\n",
    "    likelihood = quantity / len(training_set)\n",
    "    for word in test_statement:\n",
    "        if word in dataset:\n",
    "            # Add 1 smoothing\n",
    "            l_w0 = 1\n",
    "            if (word, label) in likelihood_map_data:\n",
    "                l_w0 += likelihood_map_data[(word, label)]\n",
    "            likelihood *= (l_w0 / (likelihood + len(dataset))) \n",
    "        results[label] = likelihood\n",
    "\n",
    "print(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Logistic Regression (2 points)\n",
    "\n",
    "Create an implementation (from scratch!) of the Stochastic Gradient Descent\n",
    "Algorithm in Figure 5.5 (Page 86) of the reference book “Speech and Language\n",
    "Processing”.\n",
    "\n",
    "• Note: There are some implementations online. You can use them as a reference\n",
    "but you should try to modify the implementation and show in your report that you\n",
    "understand every step. Also, if you use an implementation online as a reference\n",
    "include in your report which reference you used. Include in your report an example\n",
    "run in the style of the Example in Section 5.4.3 (Page 87) of the reference book.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "We start by initialising variables <code>weights</code> and <code>bias</code>. Our goal is to alter these variables in such a way that given any input vector <code>x</code>, our prediction function will return a value close to the <code>y</code> we've also defined. We do this by performing gradient descent on our <code>weights</code> and <code>bias</code> variables. We make a prediction using our current values. The values are then altered, as to give a closer result in our next 'training' iteration. After doing this a few iterations, we see that our prediction start to get close to our true <code>y</code> variable value. We also see that our <code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_pred):\n",
    "    return -y*math.log(y_pred) + (1 - y) * math.log(1 - y_pred)\n",
    "\n",
    "def predict(x, w, b):\n",
    "    vector_sum = -(sum([x0 * w0 for x0, w0 in zip(x, w) ]) + b)\n",
    "    return 1 / (1 + math.exp(vector_sum))\n",
    "\n",
    "def calculate_gradient(y, x, w, b, loss_func=cross_entropy_loss):\n",
    "    y_pred = predict(x, w, b)\n",
    "    loss = y_pred - y #loss_func(y, y_pred)\n",
    "    gradients = [x0 * loss for x0 in x]\n",
    "    return gradients, loss\n",
    "\n",
    "def fit(data, labels, w, b, lr):\n",
    "    for x, y in zip(data, labels):\n",
    "        gradients, loss = calculate_gradient(y, x, w, b)\n",
    "        w = [w0 - lr * g0 for w0, g0 in zip(w, gradients)]\n",
    "        b = b - lr * loss\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give as input either 1 or 0\n",
      "Weights:  [0.6000181591474809, 1.7000136193606106]  | Bias:  0.09999546021312977\n",
      "Prediction:  0.0006108073029147321 \n",
      "\n",
      "Weights:  [-0.7451029239257538, 0.6911728070556845]  | Bias:  0.43627573098143846\n",
      "Prediction:  0.7930106002028372 \n",
      "\n",
      "Weights:  [-0.9764140671491902, 0.5176894496381073]  | Bias:  0.4941035167872976\n",
      "Prediction:  0.9451474067348564 \n",
      "\n",
      "Weights:  [-1.0637405314093933, 0.45219460144295476]  | Bias:  0.5159351328523485\n",
      "Prediction:  0.9681488838019373 \n",
      "\n",
      "Weights:  [-1.1188925765172535, 0.41083056761205955]  | Bias:  0.5297231441293138\n",
      "Prediction:  0.977528986324223 \n",
      "\n",
      "Weights:  [-1.1593306992521488, 0.38050197556088805]  | Bias:  0.5398326748130375\n",
      "Prediction:  0.9826327558841839 \n",
      "\n",
      "Weights:  [-1.1912859749042366, 0.35653551882182233]  | Bias:  0.5478214937260595\n",
      "Prediction:  0.9858439898442994 \n",
      "\n",
      "Weights:  [-1.2177121362219545, 0.33671589783353373]  | Bias:  0.554428034055489\n",
      "Prediction:  0.9880514977691122 \n",
      "\n",
      "Weights:  [-1.240245524557702, 0.31981585658172335]  | Bias:  0.5600613811394257\n",
      "Prediction:  0.9896626022450891 \n",
      "\n",
      "Weights:  [-1.2598883700953822, 0.30508372242846304]  | Bias:  0.564972092523846\n",
      "Prediction:  0.9908903934582164 \n",
      "\n",
      "Weights:  [-1.277299392873571, 0.2920254553448216]  | Bias:  0.569324848218393\n",
      "Prediction:  0.9918572081538556 \n",
      "\n",
      "Weights:  [-1.2929348711563302, 0.28029884663275223]  | Bias:  0.5732337177890827\n",
      "Prediction:  0.9926383087609962 \n",
      "\n",
      "Weights:  [-1.307123971251676, 0.269657021561243]  | Bias:  0.5767809928129192\n",
      "Prediction:  0.9932825451025583 \n",
      "\n",
      "Weights:  [-1.3201119930218923, 0.25991600523358094]  | Bias:  0.580027998255473\n",
      "Prediction:  0.9938230095088545 \n",
      "\n",
      "Weights:  [-1.332086652893199, 0.2509350103301011]  | Bias:  0.5830216632232996\n",
      "Prediction:  0.9942829179275743 \n",
      "\n",
      "Weights:  [-1.343194812260476, 0.2426038908046432]  | Bias:  0.585798703065119\n",
      "Prediction:  0.9946790395005787 \n",
      "\n",
      "Weights:  [-1.3535535390281497, 0.23483484572888796]  | Bias:  0.5883883847570374\n",
      "Prediction:  0.9950237895760629 \n",
      "\n",
      "Weights:  [-1.3632576608526814, 0.22755675436048917]  | Bias:  0.5908144152131704\n",
      "Prediction:  0.9953265568761674 \n",
      "\n",
      "Weights:  [-1.3723850662678223, 0.2207112002991335]  | Bias:  0.5930962665669556\n",
      "Prediction:  0.9955945730054282 \n",
      "\n",
      "Weights:  [-1.3810005144759445, 0.2142496141430419]  | Bias:  0.5952501286189861\n",
      "Prediction:  0.9958334984399151 \n",
      "\n",
      "Weights:  [-1.3891584306341422, 0.20813117702439354]  | Bias:  0.5972896076585356\n",
      "Prediction:  0.9960478273064411 \n",
      "\n",
      "Weights:  [-1.3969049945378553, 0.20232125409660873]  | Bias:  0.5992262486344638\n",
      "Prediction:  0.9962411731321926 \n",
      "\n",
      "Weights:  [-1.4042797267979454, 0.19679020490154117]  | Bias:  0.6010699316994863\n",
      "Prediction:  0.9964164744974556 \n",
      "\n",
      "Weights:  [-1.411316710970188, 0.19151246677235925]  | Bias:  0.602829177742547\n",
      "Prediction:  0.9965761456188962 \n",
      "\n",
      "Weights:  [-1.4180455475297193, 0.18646583935271094]  | Bias:  0.6045113868824297\n",
      "Prediction:  0.9967221883345078 \n",
      "\n",
      "Weights:  [-1.4244921073464647, 0.18163091949015192]  | Bias:  0.606123026836616\n",
      "Prediction:  0.996856276561287 \n",
      "\n",
      "Weights:  [-1.4306791332018456, 0.17699065009861617]  | Bias:  0.6076697833004614\n",
      "Prediction:  0.9969798208100854 \n",
      "\n",
      "Weights:  [-1.4366267247057074, 0.17252995647071992]  | Bias:  0.6091566811764269\n",
      "Prediction:  0.9970940180440897 \n",
      "\n",
      "Weights:  [-1.4423527327298977, 0.1682354504525771]  | Bias:  0.6105881831824747\n",
      "Prediction:  0.9971998906241493 \n",
      "\n",
      "Weights:  [-1.4478730828945718, 0.16409518782907137]  | Bias:  0.6119682707236432\n",
      "Prediction:  0.997298317029953 \n",
      "\n",
      "Weights:  [-1.4532020428922998, 0.16009846783077547]  | Bias:  0.6133005107230751\n",
      "Prediction:  0.9973900563145237 \n",
      "\n",
      "Weights:  [-1.458352444960398, 0.15623566627970184]  | Bias:  0.6145881112400997\n",
      "Prediction:  0.9974757677344951 \n",
      "\n",
      "Weights:  [-1.4633358722403313, 0.15249809581975182]  | Bias:  0.6158339680600831\n",
      "Prediction:  0.997556026631233 \n",
      "\n",
      "Weights:  [-1.4681628158387305, 0.14887788812095235]  | Bias:  0.6170407039596829\n",
      "Prediction:  0.9976313373724913 \n",
      "\n",
      "Weights:  [-1.4728428079498546, 0.14536789403760947]  | Bias:  0.6182107019874639\n",
      "Prediction:  0.9977021439704076 \n",
      "\n",
      "Weights:  [-1.477384535289046, 0.14196159853321574]  | Bias:  0.6193461338222618\n",
      "Prediction:  0.9977688388484859 \n",
      "\n",
      "Weights:  [-1.4817959362317963, 0.13865304782615298]  | Bias:  0.6204489840579493\n",
      "Prediction:  0.9978317701234288 \n",
      "\n",
      "Weights:  [-1.4860842843891815, 0.1354367867081143]  | Bias:  0.6215210710972956\n",
      "Prediction:  0.9978912476873055 \n",
      "\n",
      "Weights:  [-1.490256260830958, 0.13230780437678188]  | Bias:  0.6225640652077398\n",
      "Prediction:  0.9979475483144901 \n",
      "\n",
      "Weights:  [-1.4943180167580816, 0.12926148743143928]  | Bias:  0.6235795041895206\n",
      "Prediction:  0.9980009199710719 \n",
      "\n",
      "Weights:  [-1.4982752281013376, 0.12629357892399729]  | Bias:  0.6245688070253346\n",
      "Prediction:  0.9980515854683641 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [1, 2]\n",
    "bias = 0\n",
    "learning_rate = 0.1\n",
    "x = [-4, -3]\n",
    "print('Give as input either 1 or 0')\n",
    "y = 1\n",
    "\n",
    "for i in range(200):\n",
    "    weights, bias = fit([x], [y], weights, bias, learning_rate)\n",
    "    if i == 0 or i % 5 == 4:\n",
    "        print('Weights: ', weights, ' | Bias: ', bias)\n",
    "        print('Prediction: ', predict(x, weights, bias), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
